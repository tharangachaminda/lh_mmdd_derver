# Server Settings
PORT=3000
NODE_ENV=development

# Llama Model Settings (for LangChain provider - optional)
LLAMA_MODEL_PATH=/path/to/your/llama/model.gguf
LLAMA_NUM_THREADS=4
LLAMA_CONTEXT_SIZE=2048
LLAMA_BATCH_SIZE=512

# Ollama Configuration (primary LLM provider)
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL_NAME=llama2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# OpenSearch Configuration (Vector Database)
OPENSEARCH_NODE=https://localhost:9200
OPENSEARCH_USERNAME=admin
OPENSEARCH_PASSWORD="h7F!q9rT#4vL"

# Vector Database Settings
VECTOR_INDEX_NAME=enhanced_questions
VECTOR_DIMENSION=1536
VECTOR_SIMILARITY=cosine

# Embedding Service Configuration
EMBEDDING_BATCH_SIZE=10
EMBEDDING_TIMEOUT=30000

# Language Model Provider Selection (optional)
# LLM_PROVIDER=ollama  # Options: ollama, langchain

# Data Persistence (for reference - actual paths in docker-compose.yml)
# OPENSEARCH_DATA_PATH=./opensearch-data
# OPENSEARCH_LOGS_PATH=./opensearch-logs
